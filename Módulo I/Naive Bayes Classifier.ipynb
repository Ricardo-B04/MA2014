{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSt3p4nxjxsY"
   },
   "source": [
    "# Detecting SPAM emails with a Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes Classifier is based on one of the most important results in Statistics: The Bayes Theorem. We will see how this theorem can be employed to determine if an email is SPAM or not.\n",
    "\n",
    "First, we need to load some important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "id": "QYp-SQfXjxsa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ricardob./Library/CloudStorage/OneDrive-InstitutoTecnologicoydeEstudiosSuperioresdeMonterrey/Ciencia de Datos/Métodos de Razonamiento e Incertidumbre/MA2014/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages (run once in the notebook)\n",
    "%pip install pandas\n",
    "\n",
    "## General Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6VWrV7rjxsb"
   },
   "source": [
    "Now we need to load the data we will work with. This data can be downloaded from `https://www.kaggle.com/uciml/sms-spam-collection-dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "zum7OtJ5jxsb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsMS2vbAjxsb"
   },
   "source": [
    "## Cleaning the data\n",
    "\n",
    "Before going any further, it is clear that our data needs some cleaning. For instance, the **unnamed columns** can be removed. Speaking of columns, some \"renaming\" would be desirable for the sake of clarity. Also, we would like use a \"binary variable\" for categorizing the emails: 0 for **not spam** and 1 for **spam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "hgeg8erGjxsb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados encontrados: 403\n",
      "Filas después de eliminar duplicados: 5169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               email  spam\n",
       "0  Go until jurong point, crazy.. Available only ...     0\n",
       "1                      Ok lar... Joking wif u oni...     0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3  U dun say so early hor... U c already then say...     0\n",
       "4  Nah I don't think he goes to usf, he lives aro...     0"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = data\n",
    "data_clean['spam'] = data_clean['v1'].map({'ham' : 0, 'spam' : 1})\n",
    "data_clean = data_clean.drop(columns=['v1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "data_clean = data_clean.rename(columns={'v2' : 'email'})\n",
    "# Eliminar duplicados basados en la columna 'email'\n",
    "total_duplicates = data_clean.duplicated(subset='email').sum()\n",
    "unique_dup_emails = data_clean[data_clean.duplicated(subset='email', keep=False)].copy()\n",
    "\n",
    "# Mantener solo la primera ocurrencia de cada email\n",
    "data_clean = data_clean.drop_duplicates(subset='email', keep='first').reset_index(drop=True)\n",
    "\n",
    "print(f\"Duplicados encontrados: {total_duplicates}\")\n",
    "print(f\"Filas después de eliminar duplicados: {data_clean.shape[0]}\")\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3Zoii_8jxsc"
   },
   "source": [
    "It looks nicer, doesn't it? But this is just the beggining. At this point we need to process the emails and turn them into something that our model will \"digest\" much more easily. In order to do this we need some **Natural Language Processing** (NLP): \"NLP is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data,\" according to Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dV4qEmkBjxsc"
   },
   "source": [
    "## Text Processing\n",
    "\n",
    "Text preprocessing is crucial before building a proper NLP model. Here are the important steps we are going to carry out:\n",
    "\n",
    "1. Converting words to lower case.\n",
    "2. Removing special characters.\n",
    "3. Removing stopwords.\n",
    "4. Stemming and lemmatization.\n",
    "\n",
    "More on steps three and four later. For now let us proceed with step number one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df9YBP1tjxsc"
   },
   "source": [
    "### Lower case and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "id": "lbFV9rPsjxsc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>will ì_ b going to esplanade fr home?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>pity, * was in mood for that. so...any other s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>the guy did some bitching but i acted like i'd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>rofl. its true to its name</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "0     go until jurong point, crazy.. available only ...     0\n",
       "1                         ok lar... joking wif u oni...     0\n",
       "2     free entry in 2 a wkly comp to win fa cup fina...     1\n",
       "3     u dun say so early hor... u c already then say...     0\n",
       "4     nah i don't think he goes to usf, he lives aro...     0\n",
       "...                                                 ...   ...\n",
       "5164  this is the 2nd time we have tried 2 contact u...     1\n",
       "5165              will ì_ b going to esplanade fr home?     0\n",
       "5166  pity, * was in mood for that. so...any other s...     0\n",
       "5167  the guy did some bitching but i acted like i'd...     0\n",
       "5168                         rofl. its true to its name     0\n",
       "\n",
       "[5169 rows x 2 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['email'] = data_clean['email'].apply(lambda x : x.lower())\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWI7xybLjxsc"
   },
   "source": [
    "Let us do step number two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "6VzclZsYjxsd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go until jurong point  crazy  available only i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar  joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say so early hor  u c already then say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah i don t think he goes to usf  he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>will   b going to esplanade fr home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>pity    was in mood for that  so any other sug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>the guy did some bitching but i acted like i d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>rofl  its true to its name</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "0     go until jurong point  crazy  available only i...     0\n",
       "1                             ok lar  joking wif u oni      0\n",
       "2     free entry in 2 a wkly comp to win fa cup fina...     1\n",
       "3         u dun say so early hor  u c already then say      0\n",
       "4     nah i don t think he goes to usf  he lives aro...     0\n",
       "...                                                 ...   ...\n",
       "5164  this is the 2nd time we have tried 2 contact u...     1\n",
       "5165               will   b going to esplanade fr home      0\n",
       "5166  pity    was in mood for that  so any other sug...     0\n",
       "5167  the guy did some bitching but i acted like i d...     0\n",
       "5168                         rofl  its true to its name     0\n",
       "\n",
       "[5169 rows x 2 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['email'] = data_clean['email'].apply(lambda x : re.sub('[^a-z0-9 ]+', ' ', x))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fq4uCVpjxsd"
   },
   "source": [
    "Notice that we have assumed that it is \"safe\" to turn the characters of the emails into lower case letters and that special characters do not posses relevant information. This may be okay for this type of application, but for, say, sentiment analysis, we might need to reconsider this since special characters like exclamation points are used to convey certain emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp8IzYtrjxsd"
   },
   "source": [
    "### Stop words\n",
    "\n",
    "At this point you migh be wondering \"what are stop words?\" Well, these are words that are encountered very frequently in a given language but do not carry useful information, thus it is a good practice to remove them. Before doing this, let us take a look into the stop words of the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "O9Zt7z1ojxsd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ricardob./nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7cVg-wmjxsd"
   },
   "source": [
    "Now onto removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "VA6lOkxbjxsd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ricardob./nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "id": "e0_dRNjGjxsd"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(message):\n",
    "\n",
    "    words = word_tokenize(message)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "id": "SOovjZY9jxsd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, 750, poun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>[b, going, esplanade, fr, home]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>[pity, mood, suggestions]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>[guy, bitching, acted, like, interested, buyin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "0     [go, jurong, point, crazy, available, bugis, n...     0\n",
       "1                        [ok, lar, joking, wif, u, oni]     0\n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...     1\n",
       "3         [u, dun, say, early, hor, u, c, already, say]     0\n",
       "4        [nah, think, goes, usf, lives, around, though]     0\n",
       "...                                                 ...   ...\n",
       "5164  [2nd, time, tried, 2, contact, u, u, 750, poun...     1\n",
       "5165                    [b, going, esplanade, fr, home]     0\n",
       "5166                          [pity, mood, suggestions]     0\n",
       "5167  [guy, bitching, acted, like, interested, buyin...     0\n",
       "5168                                 [rofl, true, name]     0\n",
       "\n",
       "[5169 rows x 2 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the punkt tokenizer is available (download if necessary)\n",
    "try:\n",
    "\tnltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "\tnltk.download('punkt_tab')\n",
    "\n",
    "data_clean['email'] = data_clean['email'].apply(remove_stop_words)\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFvQp8rbjxsd"
   },
   "source": [
    "Notice that apart from removing stop words we did something else, that \"something else\" is called **tokenization**: Tokenization is defined as splitting a text into small units known as **tokens**. We might think that this is as simple as taking a text and each time we find a space between words we split there, but the process is more involved than that. The method `word_tokenize` is clever enough to do thing such as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "JKAKi5m_jxse"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There', \"'s\", 'something', 'I', \"'d\", 'like', 'to', 'know', ',', 'dude', '.']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"There's something I'd like to know, dude.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scGB9sbAjxse"
   },
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "It is natural that in any language we will use variations of the same word, e.g., \"run\", \"ran\", and \"running\". These variations are called **inflections**. Even more, there are words that have similar meanings such as \"democracy\", \"democratic\", and \"democratization\". The goal of both stemming and lemmatization is to turn either inflections or derivationally related forms of a word into a common base form. For instance:\n",
    "\n",
    "*Lemmatization:* am, are, is $\\Rightarrow$ be.\n",
    "\n",
    "*Stemming:* car, cars $\\Rightarrow$ car.\n",
    "\n",
    "Stemming is considered a crude heuristic process that chops off parts of a word by taking into account common prefixes and suffixes. On the other hand, lemmatization takes into consideration the grammar of the word and attemps to find the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "id": "06d9RfQHjxse",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "car\n",
      "be\n",
      "be\n",
      "be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ricardob./nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## modules for\n",
    "## stemming and lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "Porter = PorterStemmer()\n",
    "Lemma = WordNetLemmatizer()\n",
    "\n",
    "print(Porter.stem(\"car\"))\n",
    "print(Porter.stem(\"cars\"))\n",
    "\n",
    "print(Lemma.lemmatize(\"am\", wordnet.VERB))\n",
    "print(Lemma.lemmatize(\"are\", wordnet.VERB))\n",
    "print(Lemma.lemmatize(\"is\", wordnet.VERB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85m3r7q2jxse"
   },
   "source": [
    "In the meantime, for this application, we will stick to *stemming*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "id": "n6Frj99tjxse"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nah, think, goe, usf, live, around, though]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>[2nd, time, tri, 2, contact, u, u, 750, pound,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>[b, go, esplanad, fr, home]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>[piti, mood, suggest]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>[guy, bitch, act, like, interest, buy, someth,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "0     [go, jurong, point, crazi, avail, bugi, n, gre...     0\n",
       "1                          [ok, lar, joke, wif, u, oni]     0\n",
       "2     [free, entri, 2, wkli, comp, win, fa, cup, fin...     1\n",
       "3         [u, dun, say, earli, hor, u, c, alreadi, say]     0\n",
       "4          [nah, think, goe, usf, live, around, though]     0\n",
       "...                                                 ...   ...\n",
       "5164  [2nd, time, tri, 2, contact, u, u, 750, pound,...     1\n",
       "5165                        [b, go, esplanad, fr, home]     0\n",
       "5166                              [piti, mood, suggest]     0\n",
       "5167  [guy, bitch, act, like, interest, buy, someth,...     0\n",
       "5168                                 [rofl, true, name]     0\n",
       "\n",
       "[5169 rows x 2 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['email'] = data_clean['email'].apply(lambda x : [Porter.stem(word) for word in x])\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYD2cdmDjxse"
   },
   "source": [
    "## Training and testing sets\n",
    "\n",
    "When we are developing a model we do not use all of our data for training, what we do is that we divide the data we posses into two sets: the training set and the testing set. A general rule of thumb is to use 80% of the data for training and 20% for testing our model. There are variations of this depending on the circumstances, but, in general, this is a good starting point. By the way, all the examples of our training data should be picked randomly to avoid any bias; it is not a good practice to pick these examples in a deterministic fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "OuVI6lk_jxse"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4135, 2)\n",
      "(1034, 2)\n"
     ]
    }
   ],
   "source": [
    "train_set = data_clean.sample(frac=0.8, random_state=1337)\n",
    "test_set = data_clean.drop(train_set.index)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Btm1g_S6jxse"
   },
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "You probably remember something called \"conditional probability.\" Let us assume we have two events A and B that migh be related. Also, suppose that we know that event B has occured, then we might ask what is the probability that event A occurs given that event B already happened. This is written in mathematical terms as follows: $P(A|B)$. This quantity is equal to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A|B)=\\frac{P(A\\cap B)}{P(B)}\n",
    "\\end{align}\n",
    "$$.\n",
    "\n",
    "By the way, when events A and B are independent, we have that $P(A|B)=P(A)$; this means that the ocurrence of B does not influence whatsoever the probability of A. The latter implies that $P(A\\cap B)=P(A)P(B)$.\n",
    "\n",
    "On the other hand, we could also ask what is the probability that B occurs given that A happened:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(B|A)=\\frac{P(A\\cap B)}{P(A)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, we have that $P(A|B)P(B)=P(B|A)P(A)=P(A\\cap B)$. Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This last expression is known as **Bayes' Theorem**.\n",
    "\n",
    "The term $P(A|B)$ is known as the *posterior probability*, the term $P(A)$ is defined as *a prior probability*, $P(B)$ is a *marginal probability*, and $P(B|A)$ is a conditional probability that can be understood as the likelihood of A given a fixed B: $L(A|B)=P(B|A)$.\n",
    "\n",
    "Let us see Bayes' Theorem in action. Say there is a rare disease that just one out of a thousand people has it. Also, assume there is test for this disease that identifies correctly 99% of the time the people that have the disease. Then, if a person tests positive, what is the probability that this person has the disease?\n",
    "\n",
    "Let us define two events: D is the event of a person having the disease, T is the event that a test gives a positive result. Then, to answer the question we just asked, we need to compute $P(D|T)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(D|T)=\\frac{P(T|D)P(D)}{P(T)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To begin with, we have that $P(D)=0.001$ and $P(T|D)=0.99$. As for $P(T)$, this can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(T)&=P(T|D)P(D)+P(T|\\bar{D})P(\\bar{D})\\\\\n",
    "\\\\\n",
    "&=(0.99)(0.001)+(0.01)(0.999)\\\\\n",
    "\\\\\n",
    "&=0.01098.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(D|T)=\\frac{(0.99)(0.001)}{0.01098}=0.09016...\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It is worth to consider the following situation: So our hypothetical person realized that the probability of having the disease is not that high, so he/she goes to another lab and takes the test again. If the result is, once again, positive, what is the probability that the person has the disease?\n",
    "\n",
    "In this case, our prior probability $P(D)$ is no longer 0.001 but 0.09016. Thus, we have to update both the posterior probability $P(D|T)$ and the marginal probability $P(T)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(D|T)&=\\frac{P(T|D)P(D)}{P(T)}\\\\\n",
    "\\\\\n",
    "&=\\frac{P(T|D)P(D)}{P(T|D)P(D)+P(T|\\bar{D})P(\\bar{D})}\\\\\n",
    "\\\\\n",
    "&=\\frac{(0.99)(0.0916)}{(0.99)(0.0916)+(0.01)(0.9098)}\\\\\n",
    "\\\\\n",
    "&=0.9075.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As we can see, our hypothetical character should be worried now.\n",
    "\n",
    "By the way, the latter example was taken from https://www.youtube.com/watch?v=R13BD8qKeTg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBZM2-rwjxse"
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Let us talk about emails now. Let $W$ be the set of all English words and let an email $m$ be a set of words that belong to $W$: $m=\\{w_1,w_2,\\dots,w_n\\}$. If we want to know what is the probability that said email $m$ is spam we can use, as expected, Bayes' Theorem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(spam|m)&=\\frac{P(m|spam)P(spam)}{P(m)}\\\\\n",
    "\\\\\n",
    "&=\\frac{P(w_1\\cap w_2\\cap\\cdots\\cap w_n|spam)P(spam)}{P(w_1\\cap w_2\\cap\\cdots\\cap w_n)}\\\\\n",
    "\\\\\n",
    "&=\\frac{P(w_1\\cap w_2\\cap\\cdots\\cap w_n|spam)P(spam)}{P(w_1\\cap w_2\\cap\\cdots\\cap w_n|spam)P(spam)+P(w_1\\cap w_2\\cap\\cdots\\cap w_n|not~spam)P(not~spam)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "At this point it is a good idea to focus our attention on the numerator of the last expression. Notice that we have $P(w_1\\cap w_2\\cap\\cdots\\cap w_n|spam)P(spam)$, which is equivalent to the joint probability distribution of $P(w_1\\cap w_2\\cap\\cdots\\cap w_n\\cap spam)$. By the multiplication rule, this expression can be rewritten as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1\\cap w_2\\cap\\cdots\\cap w_n\\cap spam) = P(spam)P(w_1|spam)P(w_2|w_1\\cap spam)\\cdots P(w_n|\\cap_{i=1}^{n-1}w_i\\cap spam).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And here it comes the \"naive assumption\": given the spam category, we assume that all features of the model, in this case the words of the email, are **mutually and conditionally independent** on the spam category:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_i|w_{1}\\cap\\cdots\\cap w_{i-1}\\cap spam) = P(w_i|spam).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "What this expression is telling us is that the probability of having word $w_i$ in a spam message is not affected by the presence of the set of words $\\{w_{1},\\dots,w_{i-1}\\}$ in said message, what we just need to consider is that such email is spam. Consider the sentence \"we need your info\" and assume that we know we are dealing with an email that is spam. Then, if the naive assumption is true, this could happen:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\text{need}|\\text{we}\\cap\\text{your}\\cap\\text{info}\\cap spam) = P(\\text{need}|spam).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, this is not usually true, what we have, in general, is this:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\text{need}|\\text{we}\\cap\\text{your}\\cap\\text{info}\\cap spam) \\neq P(\\text{need}|spam).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For this reason we say that this assumption is naive. Nevertheless, in practice, this classifier works very well in many situations.\n",
    "\n",
    "Let us go back to the numerator. Taking into account our naive premise, the joint probability distribution can be expressed as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1\\cap w_2\\cap\\cdots\\cap w_n\\cap spam) = P(spam)P(w_1|spam)P(w_2|spam)\\cdots P(w_n|spam).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the probability that a given message $m=\\{w_1,w_2,\\dots,w_n\\}$ is spam can be computed with this expression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(spam|w_1\\cap w_2\\cap\\cdots\\cap w_n) = \\frac{P(w_1|spam)P(w_2|spam)\\cdots P(w_n|spam)P(spam)}{P(w_1\\cap w_2\\cap\\cdots\\cap w_n)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You migh be asking, well, how can we classify an email as spam with all this? There are two options: the **Probabilistic Model** and the **Maximum A Posteriori Model (MAP)**.\n",
    "\n",
    "#### Probabilistic Model\n",
    "\n",
    "Given a threshold $p$, we classify an email as spam if this condition holds:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(spam|w_1\\cap w_2\\cap\\cdots\\cap w_n) > p.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Maximum A Posteriori Model (MAP)\n",
    "\n",
    "An email is categorized as spam if\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(spam|w_1\\cap w_2\\cap\\cdots\\cap w_n) > P(not~spam|w_1\\cap w_2\\cap\\cdots\\cap w_n),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1|spam)P(w_2|spam)\\cdots P(w_n|spam)P(spam) > P(w_1|not~spam)P(w_2|not~spam)\\cdots P(w_n|not~spam)P(not~spam).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice that it is not necessary to calculate $P(w_1\\cap w_2\\cap\\cdots\\cap w_n)$. For classifying emails we will employ this method.\n",
    "\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "Let $W_{\\text{t}}$ be the set that contains all the words of the emails that belong to the training set. As expected, $W_{\\text{t}}=W_{\\text{t-~s}}~\\cup W_{\\text{t-s}}$ and $W_{\\text{t-~s}}~\\cap W_{\\text{t-s}}=\\emptyset$, where $W_{\\text{t-~s}}~$ and $W_{\\text{t-s}}~$ are the subsets of the training set that contain non-spam and spam emails, respectively. In the training phase we need to compute the following probabilities for the training set:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_i|spam), & ~\\forall w_i\\in W_{\\text{t-s}}\\\\\n",
    "\\\\\n",
    "P(w_i|not~spam), & ~\\forall w_i\\in W_{\\text{t-~s}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_i|spam)=\\frac{\\text{number of ocurrences of $w_i$ in spam emails}}{\\text{total number of words of spam emails}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_i|not~spam)=\\frac{\\text{number of ocurrences of $w_i$ in non-spam emails}}{\\text{total number of words of non-spam emails}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Also, we need to calculate $P(spam)$ and $P(not~spam)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(spam)&=\\frac{|W_{\\text{t-s}}~|}{|W_{\\text{t}}|}\\\\\n",
    "\\\\\n",
    "P(not~spam)&=\\frac{|W_{\\text{t-~s}}~~|}{|W_{\\text{t}}|}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By the way, this way of computing the probabilities is based on the **Bag of Words** model, in which we are interested in the frequencies of each of the words of a corpus without taking into consideration neither grammar  nor order.\n",
    "\n",
    "This is not the only model at our disposal, another popular option is the **Term Frequency-Inverse Document Frequency (TF-IDF)** model, which is based on information theory. For now, we will focus on the bag-of-words approach, but if you want to know more this is a good starting point: https://en.wikipedia.org/wiki/Tf–idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24f8bhzrjxsf",
    "outputId": "2c363ecb-2306-4be1-ff74-bd509e23cf78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12841596130592503"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam = train_set[train_set['spam'] == 1].shape[0] / train_set.shape[0]\n",
    "p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "papvTOoHjxsf",
    "outputId": "cd4b073c-b6d3-4242-e3d0-1b60e8ddf7aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.871584038694075"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_not_spam = train_set[train_set['spam'] == 0].shape[0] / train_set.shape[0]\n",
    "p_not_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "id": "sgIiTFbcjxsg"
   },
   "outputs": [],
   "source": [
    "def bag_of_words(corpus):\n",
    "\n",
    "    \"\"\"\n",
    "    This function receives a corpus, i.e., the set of processed emails, and\n",
    "    returns a dictionary in which each item is a unique word and each word\n",
    "    has its corresponding number of ocurrences in the corpus.\n",
    "    \"\"\"\n",
    "    bag_of_words = {}\n",
    "\n",
    "    for message in corpus:\n",
    "        for word in message:\n",
    "            if word in bag_of_words:\n",
    "                bag_of_words[word] += 1\n",
    "            else:\n",
    "                bag_of_words[word] = 1\n",
    "\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "id": "VBhhQK68jxsg"
   },
   "outputs": [],
   "source": [
    "def probability_words(df, vocab_size=None, alpha=1):\n",
    "\n",
    "    \"\"\"\n",
    "    This function receives a dataframe of either spam emails or non-spam emails\n",
    "    that has been processed as shown above. Using the dictionary that is returned\n",
    "    by the previous function and the data contained in df, this function computes\n",
    "    the probability of each word in bag_of_words. \n",
    "    \n",
    "    WITH LAPLACE SMOOTHING: Adds alpha (default=1) to each count to avoid zero \n",
    "    probabilities for unseen words.\n",
    "    Formula: P(w|class) = (count(w) + alpha) / (total_words + alpha * vocab_size)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Contains 'email' column with processed emails\n",
    "    vocab_size : int\n",
    "        Total unique words in vocabulary (if None, uses only words in this set)\n",
    "    alpha : float (default=1)\n",
    "        Laplace smoothing parameter\n",
    "    \"\"\"\n",
    "\n",
    "    probability_words = {}\n",
    "    \n",
    "    # Get the bag of words from the email column\n",
    "    bow = bag_of_words(df['email'])\n",
    "    \n",
    "    # Calculate total number of words\n",
    "    total_words = sum(bow.values())\n",
    "    \n",
    "    # If vocab_size is provided, apply Laplace Smoothing\n",
    "    if vocab_size is not None:\n",
    "        for word in vocab_all:\n",
    "            count = bow.get(word, 0)\n",
    "            probability_words[word] = (count + alpha) / (total_words + alpha * vocab_size)\n",
    "    else:\n",
    "        # Original behavior: only words in this class\n",
    "        for word, count in bow.items():\n",
    "            probability_words[word] = count / total_words\n",
    "\n",
    "    return probability_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "QmpWn_sgjxsh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 6324 palabras únicas\n",
      "Palabras en spam: 2284\n",
      "Palabras en no-spam: 4868\n",
      "\n",
      "Probabilidades calculadas con Laplace Smoothing ✓\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Build the complete vocabulary from both spam and non-spam emails\n",
    "spam_emails = train_set[train_set['spam'] == 1]\n",
    "non_spam_emails = train_set[train_set['spam'] == 0]\n",
    "\n",
    "spam_bow = bag_of_words(spam_emails['email'])\n",
    "non_spam_bow = bag_of_words(non_spam_emails['email'])\n",
    "\n",
    "# Combine vocabularies\n",
    "vocab_all = set(spam_bow.keys()) | set(non_spam_bow.keys())\n",
    "vocab_size = len(vocab_all)\n",
    "\n",
    "print(f\"Tamaño del vocabulario: {vocab_size} palabras únicas\")\n",
    "print(f\"Palabras en spam: {len(spam_bow)}\")\n",
    "print(f\"Palabras en no-spam: {len(non_spam_bow)}\")\n",
    "\n",
    "# Calculate probabilities WITH Laplace Smoothing\n",
    "probability_spam_words = probability_words(spam_emails, vocab_size=vocab_size, alpha=1)\n",
    "probability_non_spam_words = probability_words(non_spam_emails, vocab_size=vocab_size, alpha=1)\n",
    "\n",
    "print(\"\\nProbabilidades calculadas con Laplace Smoothing ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "x51AMoMSjxsh"
   },
   "outputs": [],
   "source": [
    "def classify_email(email, threshold=0.5):\n",
    "\n",
    "    \"\"\"\n",
    "    Improved classifier using:\n",
    "    1. Laplace Smoothing (no zero probabilities)\n",
    "    2. Log-probabilities (avoids numerical underflow)\n",
    "    3. Adjustable threshold for better spam detection\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    email : list\n",
    "        List of processed words from an email\n",
    "    threshold : float (default=0.5)\n",
    "        Decision threshold. Email is spam if P(spam|email) > threshold\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : 1 if spam, 0 if not spam\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle empty emails\n",
    "    if not email or len(email) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate log probability of spam\n",
    "    log_prob_spam = math.log(p_spam)\n",
    "    for word in email:\n",
    "        if word in probability_spam_words:\n",
    "            log_prob_spam += math.log(probability_spam_words[word])\n",
    "    \n",
    "    # Calculate log probability of non-spam\n",
    "    log_prob_non_spam = math.log(p_not_spam)\n",
    "    for word in email:\n",
    "        if word in probability_non_spam_words:\n",
    "            log_prob_non_spam += math.log(probability_non_spam_words[word])\n",
    "    \n",
    "    # Calculate posterior probability P(spam|email) using Bayes' theorem\n",
    "    # To avoid overflow: P(spam|email) = 1 / (1 + exp(log_prob_non_spam - log_prob_spam))\n",
    "    log_odds = log_prob_spam - log_prob_non_spam\n",
    "    prob_spam_posterior = 1 / (1 + math.exp(-log_odds))\n",
    "    \n",
    "    return 1 if prob_spam_posterior > threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "IRSTseGbjxsh",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimentando con diferentes thresholds:\n",
      "\n",
      "Threshold    Accuracy     Precision    Recall       F1 Score    \n",
      "------------------------------------------------------------\n",
      "0.1          0.9458       0.6964       0.9590       0.8069      \n",
      "0.2          0.9700       0.8227       0.9508       0.8821      \n",
      "0.3          0.9749       0.8529       0.9508       0.8992      \n",
      "0.4          0.9807       0.8923       0.9508       0.9206      \n",
      "0.5          0.9855       0.9280       0.9508       0.9393      \n",
      "0.6          0.9874       0.9580       0.9344       0.9461      \n",
      "0.7          0.9884       0.9741       0.9262       0.9496      \n",
      "0.8          0.9884       0.9741       0.9262       0.9496      \n",
      "0.9          0.9874       0.9823       0.9098       0.9447      \n",
      "\n",
      "============================================================\n",
      "MEJOR THRESHOLD: 0.7\n",
      "============================================================\n",
      "\n",
      "Resultados con modelo mejorado:\n"
     ]
    }
   ],
   "source": [
    "test_set_hat = test_set.copy()\n",
    "\n",
    "# Test with different thresholds to find the best one\n",
    "thresholds_to_test = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "results_by_threshold = {}\n",
    "\n",
    "print(\"Experimentando con diferentes thresholds:\\n\")\n",
    "print(f\"{'Threshold':<12} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1 Score':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    try:\n",
    "        test_set_temp = test_set.copy()\n",
    "        test_set_temp['prediction'] = test_set['email'].apply(\n",
    "            lambda x: classify_email(x, threshold=threshold)\n",
    "        )\n",
    "        \n",
    "        _, metrics_temp = performance_metrics(test_set_temp)\n",
    "        results_by_threshold[threshold] = metrics_temp\n",
    "        \n",
    "        acc = metrics_temp.loc['Accuracy', 'Metrics']\n",
    "        prec = metrics_temp.loc['Precission', 'Metrics']\n",
    "        rec = metrics_temp.loc['Recall', 'Metrics']\n",
    "        f1 = metrics_temp.loc['F1 Score', 'Metrics']\n",
    "        \n",
    "        print(f\"{threshold:<12.1f} {acc:<12.4f} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{threshold:<12.1f} ERROR: {str(e)}\")\n",
    "\n",
    "# Find best threshold based on F1 Score\n",
    "if results_by_threshold:\n",
    "    best_threshold = max(results_by_threshold.keys(), \n",
    "                          key=lambda t: results_by_threshold[t].loc['F1 Score', 'Metrics'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MEJOR THRESHOLD: {best_threshold}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Apply best threshold\n",
    "    test_set_hat['prediction'] = test_set['email'].apply(\n",
    "        lambda x: classify_email(x, threshold=best_threshold)\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: No se pudieron calcular los thresholds\")\n",
    "    best_threshold = 0.5\n",
    "    test_set_hat['prediction'] = test_set['email'].apply(\n",
    "        lambda x: classify_email(x, threshold=0.5)\n",
    "    )\n",
    "\n",
    "print(\"Resultados con modelo mejorado:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkbd7tVGjxsh"
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "So we have built the Naive Bayes Classifier and we have trained it, but is it good? To know how good our model is we need **evaluation metrics**. There are tons of metrics, and the ideal metric, or metrics, will have to be chosen depending on what is important for your particular application. For now, we will mention a few of the most common, however, before going any further, we need to say a few things about the **confusion matrix**.\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that allows us to visualize the performance of a classification algorithm.\n",
    "\n",
    "<img src=\"confusion.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "This type of table receives this name because it lets us observe whether an algorithm is mislabeling two classes (Image taken from https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "Accuracy is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy}=\\frac{\\text{true positives} + \\text{true negatives}}{\\text{true positives} + \\text{false positives} +  \\text{true negatives} + \\text{false negatives}}.\n",
    "$$\n",
    "\n",
    "This metric is useful when both classes are equally important and when we have balanced set, which is not quite the case in this application.\n",
    "\n",
    "#### Precision\n",
    "\n",
    "The ratio of positive cases that were correctly labeled over all the examples that were classified as positive is called **precision**:\n",
    "\n",
    "$$\n",
    "\\text{Precision}=\\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives}}.\n",
    "$$\n",
    "\n",
    "When we are interested in reducing the amount of false positives and we have imbalanced sets, precision is a good choice as an evaluation metric. In fact, for this application, this metric is appriopriate since we are interested in detecting spam emails: spam is the positive category, if a regular email is classified as spam (false positive), we are sending emails that are important for us to the spam folder; however, if a spam email is labeled as not-spam, said email will end up in our inbox, which is not as serious as not reading an email that we are expecting. Also, keep in mind that our sets are imbalanced: the majority of our emails in the data are not spam.\n",
    "\n",
    "#### Recall\n",
    "\n",
    "Recall is the ratio of the examples that were correclty identified as a positive case over all the true positives examples in our data. This metric can be understood as the sensitivity of our model:\n",
    "\n",
    "$$\n",
    "\\text{Recall}=\\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}.\n",
    "$$\n",
    "\n",
    "If we want to pay special attention to the false negatives that our model is detecting, and if our sets are imbalanced, then this can be one of our performance metrics. Say we want to build a model that detects a dangerous disease. In this case, we are not interested in telling a person that he/she does not have the disease when that is not the case (false negative).\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "The F1 score is equal to the harmonic mean of precision and recall. It is useful when we want to have a balance between precision and recall and when we do not have balanced sets (large number of actual negatives). It is defined as\n",
    "\n",
    "$$\n",
    "\\text{F1 Score}=2\\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "nF2N1yPXjxsh"
   },
   "outputs": [],
   "source": [
    "def performance_metrics(results):\n",
    "\n",
    "    positives = results[['spam', 'prediction']][results['spam'] == 1]\n",
    "    negatives = results[['spam', 'prediction']][results['spam'] == 0]\n",
    "\n",
    "    true_negatives = negatives[negatives['spam'] == negatives['prediction']].shape[0]\n",
    "    false_positives = negatives[negatives['spam'] != negatives['prediction']].shape[0]\n",
    "    true_positives = positives[positives['spam'] == positives['prediction']].shape[0]\n",
    "    false_negatives = positives[positives['spam'] != positives['prediction']].shape[0]\n",
    "\n",
    "    confusion_matrix = {'actual positives' : [true_positives, false_negatives],\n",
    "                        'actual negatives' : [false_positives, true_negatives]}\n",
    "\n",
    "    confusion_matrix_df = pd.DataFrame.from_dict(confusion_matrix, orient='index',\n",
    "                                                 columns=['predicted positives', 'predicted negatives'])\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives +  true_negatives + false_negatives)\n",
    "    precission = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precission * recall) / (precission + recall)\n",
    "\n",
    "    metrics = {'Accuracy' : accuracy, 'Precission' : precission, 'Recall' : recall, 'F1 Score' : f1_score}\n",
    "\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Metrics'])\n",
    "\n",
    "    return confusion_matrix_df, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "id": "ouX3rMKLjxsi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión (Modelo Mejorado):\n",
      "                  predicted positives  predicted negatives\n",
      "actual positives                  113                    9\n",
      "actual negatives                    3                  909\n",
      "\n",
      "Métricas (Modelo Mejorado):\n",
      "            Metrics\n",
      "Accuracy     0.9884\n",
      "Precission   0.9741\n",
      "Recall       0.9262\n",
      "F1 Score     0.9496\n",
      "\n",
      "Último threshold utilizado: 0.7\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix, metrics = performance_metrics(test_set_hat)\n",
    "print(\"\\nMatriz de Confusión (Modelo Mejorado):\")\n",
    "print(confusion_matrix)\n",
    "print(\"\\nMétricas (Modelo Mejorado):\")\n",
    "print(metrics)\n",
    "print(f\"\\nÚltimo threshold utilizado: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMR_uygljxsi"
   },
   "source": [
    "As we can see, our model has good precision, but its recall is poor: a lot of emails that are spam were labeled as not-spam. Although this is not a serious issue for this type of application, this suggests that we should get more examples of spam emails if we want to increase the sensitivity of our model or try different strategies such as n-grams, TF-IDF, etc., or both things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5Om7veLj_WZ"
   },
   "source": [
    "## Generating new messages\n",
    "\n",
    "It turns out that we can use the conditional distributions that we learned in the training phase to generate either spam or not spam messages. For creating an spam email we can employ this distribution:\n",
    "\n",
    "$$P(w|\\text{spam}).$$\n",
    "\n",
    "Notice that said distribution is stored in `probability_spam_words`.\n",
    "\n",
    "In the next cell, use the `np.random.choice` function and the `join` method for creating an spam message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "PSJMDjfAs1X0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Spam Email:\n",
      "hdd get enufcredeit prize payoh doesnt back sari mobil u abl claim purpos txt ymca 2 08718730555 350 termsappli box434sk38wp150ppm18\n"
     ]
    }
   ],
   "source": [
    "# Generate a spam email with a length of 20 words\n",
    "spam_words = list(probability_spam_words.keys())\n",
    "spam_probabilities = list(probability_spam_words.values())\n",
    "spam_email = ' '.join(np.random.choice(spam_words, size=20, p=spam_probabilities))\n",
    "print(\"Generated Spam Email:\")\n",
    "print(spam_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "id": "OIieVEgouT3Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Non-Spam Email:\n",
      "teas ibiza apo today decid 2mrw oh thk fr keypad question use happenin gt along merememberin wait soft happi doinat\n"
     ]
    }
   ],
   "source": [
    "# Generate a non spam email composed of 20 random words\n",
    "non_spam_words = list(probability_non_spam_words.keys())\n",
    "non_spam_probabilities = list(probability_non_spam_words.values())\n",
    "non_spam_email = ' '.join(np.random.choice(non_spam_words, size=20, p=non_spam_probabilities))\n",
    "print(\"Generated Non-Spam Email:\")\n",
    "print(non_spam_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmKjMiSbuYQP"
   },
   "source": [
    "The messages that you got should not make much sense since the we followed the \"naive\" approach and stemmed words. Nevertheless, what you get should give you an idea of the type of words you can find in these two types of emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams Strategy: Implementing Bi-grams\n",
    "\n",
    "To improve the recall of our model, we can use **n-grams** instead of just individual words (unigrams). An **n-gram** is a sequence of n consecutive words. For example:\n",
    "\n",
    "- **Unigrams** (1-gram): \"free\", \"money\", \"click\"\n",
    "- **Bi-grams** (2-gram): \"free money\", \"click here\", \"limited time\"\n",
    "- **Trigrams** (3-gram): \"free money now\", \"click here today\"\n",
    "\n",
    "Bi-grams capture sequential relationships between words, which can better capture common spam patterns like \"limited time\", \"free money\", or \"click here\". This contextual information can help improve both precision and recall.\n",
    "\n",
    "We will now implement bi-grams alongside the existing unigrams to create a more powerful feature set for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-grams generated successfully!\n",
      "\n",
      "Example email with unigrams + bigrams:\n",
      "['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor']\n"
     ]
    }
   ],
   "source": [
    "def generate_bigrams(words):\n",
    "    \"\"\"\n",
    "    Generate bi-grams from a list of words.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    words : list\n",
    "        List of stemmed words from an email\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Combined list of unigrams and bi-grams\n",
    "           e.g., [\"free\", \"money\", \"time\", \"free_money\", \"money_time\"]\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "    for i in range(len(words) - 1):\n",
    "        bigrams.append(f\"{words[i]}_{words[i+1]}\")\n",
    "    \n",
    "    # Return unigrams + bigrams combined\n",
    "    return words + bigrams\n",
    "\n",
    "\n",
    "def filter_bigrams_by_frequency(data_with_bigrams, min_frequency=3):\n",
    "    \"\"\"\n",
    "    Filter bi-grams that appear with low frequency from a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_with_bigrams : DataFrame\n",
    "        DataFrame with 'email' column containing unigrams + bigrams\n",
    "    min_frequency : int (default=3)\n",
    "        Minimum frequency threshold for keeping a bi-gram\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (filtered_bigrams_set, original_count, filtered_count, removed_bigrams)\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Count all bigrams in the dataset\n",
    "    all_bigrams = [word for email in data_with_bigrams['email'] for word in email if '_' in word]\n",
    "    bigram_counter = Counter(all_bigrams)\n",
    "    \n",
    "    # Filter bigrams by minimum frequency\n",
    "    filtered_bigrams = {bigram for bigram, count in bigram_counter.items() if count >= min_frequency}\n",
    "    \n",
    "    original_count = len(bigram_counter)\n",
    "    filtered_count = len(filtered_bigrams)\n",
    "    removed_count = original_count - filtered_count\n",
    "    removed_bigrams = {bigram: count for bigram, count in bigram_counter.items() if count < min_frequency}\n",
    "    \n",
    "    return filtered_bigrams, original_count, filtered_count, removed_bigrams\n",
    "\n",
    "\n",
    "# Apply bi-gram transformation to all emails\n",
    "data_clean['email'] = data_clean['email'].apply(generate_bigrams)\n",
    "print(\"Bi-grams generated successfully!\")\n",
    "print(f\"\\nExample email with unigrams + bigrams:\")\n",
    "print(data_clean['email'].iloc[0][:15])  # Show first 15 features (mix of unigrams and bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape (with bi-grams): (4135, 2)\n",
      "Test set shape (with bi-grams): (1034, 2)\n"
     ]
    }
   ],
   "source": [
    "## Re-split the data with bi-grams\n",
    "train_set_bigrams = data_clean.sample(frac=0.8, random_state=1337)\n",
    "test_set_bigrams = data_clean.drop(train_set_bigrams.index)\n",
    "\n",
    "print(\"Training set shape (with bi-grams):\", train_set_bigrams.shape)\n",
    "print(\"Test set shape (with bi-grams):\", test_set_bigrams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model with Bi-grams\n",
    "\n",
    "Now we'll train a new Naive Bayes classifier using the combined unigrams and bi-grams features. The process is the same as before, but now each email will have more features (both individual words and word pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTRADO DE BI-GRAMAS POR FRECUENCIA MÍNIMA\n",
      "======================================================================\n",
      "\n",
      "Min Frequency = 2:\n",
      "  Spam: 5681 → 1456 bi-gramas (23208 eliminados)\n",
      "  No-Spam: 21107 → 2124 bi-gramas\n",
      "  Total después de filtrado: 3543 bi-gramas únicos\n",
      "\n",
      "Min Frequency = 3:\n",
      "  Spam: 5681 → 628 bi-gramas (25479 eliminados)\n",
      "  No-Spam: 21107 → 681 bi-gramas\n",
      "  Total después de filtrado: 1292 bi-gramas únicos\n",
      "\n",
      "Min Frequency = 5:\n",
      "  Spam: 5681 → 240 bi-gramas (26340 eliminados)\n",
      "  No-Spam: 21107 → 208 bi-gramas\n",
      "  Total después de filtrado: 442 bi-gramas únicos\n",
      "\n",
      "Min Frequency = 10:\n",
      "  Spam: 5681 → 51 bi-gramas (26685 eliminados)\n",
      "  No-Spam: 21107 → 52 bi-gramas\n",
      "  Total después de filtrado: 103 bi-gramas únicos\n",
      "\n",
      "======================================================================\n",
      "\n",
      "USANDO MIN_FREQUENCY = 3:\n",
      "  Bi-gramas originales: 26585\n",
      "  Bi-gramas después del filtrado: 1335\n",
      "  Reducción: 25250 (95.0%)\n",
      "\n",
      "Tamaño del vocabulario (con bi-gramas filtrados): 7659 características únicas\n",
      "Características en spam: 7965\n",
      "Características en no-spam: 25975\n",
      "\n",
      "Comparación:\n",
      "  Vocabulario anterior (solo unigramas): 6324\n",
      "  Vocabulario con bi-gramas (sin filtrado): 33554\n",
      "  Vocabulario con bi-gramas (CON FILTRADO): 7659\n",
      "  Reducción gracias al filtrado: 25895 características\n",
      "\n",
      "Probabilidades calculadas con Laplace Smoothing (bi-gramas filtrados) ✓\n"
     ]
    }
   ],
   "source": [
    "# Build the complete vocabulary from both spam and non-spam emails (with bi-grams)\n",
    "spam_emails_bigrams = train_set_bigrams[train_set_bigrams['spam'] == 1]\n",
    "non_spam_emails_bigrams = train_set_bigrams[train_set_bigrams['spam'] == 0]\n",
    "\n",
    "spam_bow_bigrams = bag_of_words(spam_emails_bigrams['email'])\n",
    "non_spam_bow_bigrams = bag_of_words(non_spam_emails_bigrams['email'])\n",
    "\n",
    "# Apply frequency-based filtering to bi-grams\n",
    "print(\"FILTRADO DE BI-GRAMAS POR FRECUENCIA MÍNIMA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test different minimum frequency thresholds\n",
    "min_freq_thresholds = [2, 3, 5, 10]\n",
    "filtered_bigrams_best = None\n",
    "best_min_freq = 3\n",
    "\n",
    "for min_freq in min_freq_thresholds:\n",
    "    filtered_bigrams_spam, orig_spam, filt_spam, removed_spam = filter_bigrams_by_frequency(\n",
    "        spam_emails_bigrams, min_frequency=min_freq\n",
    "    )\n",
    "    filtered_bigrams_non_spam, orig_non_spam, filt_non_spam, removed_non_spam = filter_bigrams_by_frequency(\n",
    "        non_spam_emails_bigrams, min_frequency=min_freq\n",
    "    )\n",
    "    \n",
    "    combined_filtered = filtered_bigrams_spam | filtered_bigrams_non_spam\n",
    "    total_removed = (orig_spam - filt_spam) + (orig_non_spam - filt_non_spam)\n",
    "    \n",
    "    print(f\"\\nMin Frequency = {min_freq}:\")\n",
    "    print(f\"  Spam: {orig_spam} → {filt_spam} bi-gramas ({total_removed} eliminados)\")\n",
    "    print(f\"  No-Spam: {orig_non_spam} → {filt_non_spam} bi-gramas\")\n",
    "    print(f\"  Total después de filtrado: {len(combined_filtered)} bi-gramas únicos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Use min_frequency = 3 as default (good balance)\n",
    "filtered_bigrams_set, orig_all, filt_all, removed_all = filter_bigrams_by_frequency(\n",
    "    train_set_bigrams, min_frequency=best_min_freq\n",
    ")\n",
    "\n",
    "print(f\"\\nUSANDO MIN_FREQUENCY = {best_min_freq}:\")\n",
    "print(f\"  Bi-gramas originales: {orig_all}\")\n",
    "print(f\"  Bi-gramas después del filtrado: {filt_all}\")\n",
    "print(f\"  Reducción: {orig_all - filt_all} ({100*(orig_all - filt_all)/orig_all:.1f}%)\")\n",
    "\n",
    "# Combine vocabularies (unigrams + filtered bigrams)\n",
    "vocab_all_bigrams = set(spam_bow_bigrams.keys()) | set(non_spam_bow_bigrams.keys())\n",
    "\n",
    "# Remove bi-grams that don't meet the frequency threshold\n",
    "vocab_all_bigrams = (vocab_all_bigrams - {word for word in vocab_all_bigrams if '_' in word}) | filtered_bigrams_set\n",
    "\n",
    "vocab_size_bigrams = len(vocab_all_bigrams)\n",
    "\n",
    "print(f\"\\nTamaño del vocabulario (con bi-gramas filtrados): {vocab_size_bigrams} características únicas\")\n",
    "print(f\"Características en spam: {len(spam_bow_bigrams)}\")\n",
    "print(f\"Características en no-spam: {len(non_spam_bow_bigrams)}\")\n",
    "print(f\"\\nComparación:\")\n",
    "print(f\"  Vocabulario anterior (solo unigramas): {vocab_size}\")\n",
    "print(f\"  Vocabulario con bi-gramas (sin filtrado): {33554}\")  # From previous run\n",
    "print(f\"  Vocabulario con bi-gramas (CON FILTRADO): {vocab_size_bigrams}\")\n",
    "print(f\"  Reducción gracias al filtrado: {33554 - vocab_size_bigrams} características\")\n",
    "\n",
    "# Calculate probabilities WITH Laplace Smoothing (using filtered bi-grams)\n",
    "probability_spam_words_bigrams = probability_words(spam_emails_bigrams, vocab_size=vocab_size_bigrams, alpha=1)\n",
    "probability_non_spam_words_bigrams = probability_words(non_spam_emails_bigrams, vocab_size=vocab_size_bigrams, alpha=1)\n",
    "\n",
    "# Remove probabilities for bi-grams that were filtered out\n",
    "probability_spam_words_bigrams = {word: prob for word, prob in probability_spam_words_bigrams.items() \n",
    "                                   if not (word in vocab_all_bigrams and '_' in word and word not in filtered_bigrams_set)}\n",
    "probability_non_spam_words_bigrams = {word: prob for word, prob in probability_non_spam_words_bigrams.items() \n",
    "                                       if not (word in vocab_all_bigrams and '_' in word and word not in filtered_bigrams_set)}\n",
    "\n",
    "print(\"\\nProbabilidades calculadas con Laplace Smoothing (bi-gramas filtrados) ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_email_bigrams(email, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Classifier using bi-grams with:\n",
    "    1. Laplace Smoothing (no zero probabilities)\n",
    "    2. Log-probabilities (avoids numerical underflow)\n",
    "    3. Adjustable threshold for better spam detection\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    email : list\n",
    "        List of features (unigrams and bi-grams) from an email\n",
    "    threshold : float (default=0.5)\n",
    "        Decision threshold. Email is spam if P(spam|email) > threshold\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : 1 if spam, 0 if not spam\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle empty emails\n",
    "    if not email or len(email) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate log probability of spam\n",
    "    log_prob_spam = math.log(p_spam)\n",
    "    for feature in email:\n",
    "        if feature in probability_spam_words_bigrams:\n",
    "            log_prob_spam += math.log(probability_spam_words_bigrams[feature])\n",
    "    \n",
    "    # Calculate log probability of non-spam\n",
    "    log_prob_non_spam = math.log(p_not_spam)\n",
    "    for feature in email:\n",
    "        if feature in probability_non_spam_words_bigrams:\n",
    "            log_prob_non_spam += math.log(probability_non_spam_words_bigrams[feature])\n",
    "    \n",
    "    # Calculate posterior probability P(spam|email) using Bayes' theorem\n",
    "    log_odds = log_prob_spam - log_prob_non_spam\n",
    "    prob_spam_posterior = 1 / (1 + math.exp(-log_odds))\n",
    "    \n",
    "    return 1 if prob_spam_posterior > threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimentando con diferentes thresholds (BI-GRAMAS):\n",
      "\n",
      "Threshold    Accuracy     Precision    Recall       F1 Score    \n",
      "------------------------------------------------------------\n",
      "0.1          0.9410       0.6763       0.9590       0.7932      \n",
      "0.2          0.9671       0.8014       0.9590       0.8731      \n",
      "0.3          0.9720       0.8345       0.9508       0.8889      \n",
      "0.4          0.9778       0.8722       0.9508       0.9098      \n",
      "0.5          0.9826       0.9062       0.9508       0.9280      \n",
      "0.6          0.9874       0.9431       0.9508       0.9469      \n",
      "0.7          0.9894       0.9664       0.9426       0.9544      \n",
      "0.8          0.9884       0.9741       0.9262       0.9496      \n",
      "0.9          0.9894       0.9826       0.9262       0.9536      \n",
      "\n",
      "============================================================\n",
      "MEJOR THRESHOLD (BI-GRAMAS): 0.7\n",
      "============================================================\n",
      "\n",
      "0.4          0.9778       0.8722       0.9508       0.9098      \n",
      "0.5          0.9826       0.9062       0.9508       0.9280      \n",
      "0.6          0.9874       0.9431       0.9508       0.9469      \n",
      "0.7          0.9894       0.9664       0.9426       0.9544      \n",
      "0.8          0.9884       0.9741       0.9262       0.9496      \n",
      "0.9          0.9894       0.9826       0.9262       0.9536      \n",
      "\n",
      "============================================================\n",
      "MEJOR THRESHOLD (BI-GRAMAS): 0.7\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set_bigrams_hat = test_set_bigrams.copy()\n",
    "\n",
    "# Test with different thresholds to find the best one (using bi-grams)\n",
    "thresholds_to_test = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "results_by_threshold_bigrams = {}\n",
    "\n",
    "print(\"Experimentando con diferentes thresholds (BI-GRAMAS):\\n\")\n",
    "print(f\"{'Threshold':<12} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1 Score':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    try:\n",
    "        test_set_temp = test_set_bigrams.copy()\n",
    "        test_set_temp['prediction'] = test_set_bigrams['email'].apply(\n",
    "            lambda x: classify_email_bigrams(x, threshold=threshold)\n",
    "        )\n",
    "        \n",
    "        _, metrics_temp = performance_metrics(test_set_temp)\n",
    "        results_by_threshold_bigrams[threshold] = metrics_temp\n",
    "        \n",
    "        acc = metrics_temp.loc['Accuracy', 'Metrics']\n",
    "        prec = metrics_temp.loc['Precission', 'Metrics']\n",
    "        rec = metrics_temp.loc['Recall', 'Metrics']\n",
    "        f1 = metrics_temp.loc['F1 Score', 'Metrics']\n",
    "        \n",
    "        print(f\"{threshold:<12.1f} {acc:<12.4f} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{threshold:<12.1f} ERROR: {str(e)}\")\n",
    "\n",
    "# Find best threshold based on F1 Score\n",
    "if results_by_threshold_bigrams:\n",
    "    best_threshold_bigrams = max(results_by_threshold_bigrams.keys(), \n",
    "                                  key=lambda t: results_by_threshold_bigrams[t].loc['F1 Score', 'Metrics'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MEJOR THRESHOLD (BI-GRAMAS): {best_threshold_bigrams}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Apply best threshold\n",
    "    test_set_bigrams_hat['prediction'] = test_set_bigrams['email'].apply(\n",
    "        lambda x: classify_email_bigrams(x, threshold=best_threshold_bigrams)\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: No se pudieron calcular los thresholds\")\n",
    "    best_threshold_bigrams = 0.5\n",
    "    test_set_bigrams_hat['prediction'] = test_set_bigrams['email'].apply(\n",
    "        lambda x: classify_email_bigrams(x, threshold=0.5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión (Modelo con BI-GRAMAS):\n",
      "                  predicted positives  predicted negatives\n",
      "actual positives                  115                    7\n",
      "actual negatives                    4                  908\n",
      "\n",
      "Métricas (Modelo con BI-GRAMAS):\n",
      "            Metrics\n",
      "Accuracy     0.9894\n",
      "Precission   0.9664\n",
      "Recall       0.9426\n",
      "F1 Score     0.9544\n",
      "\n",
      "Último threshold utilizado: 0.7\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the Bi-gram Model\n",
    "\n",
    "confusion_matrix_bigrams, metrics_bigrams = performance_metrics(test_set_bigrams_hat)\n",
    "print(\"Matriz de Confusión (Modelo con BI-GRAMAS):\")\n",
    "print(confusion_matrix_bigrams)\n",
    "print(\"\\nMétricas (Modelo con BI-GRAMAS):\")\n",
    "print(metrics_bigrams)\n",
    "print(f\"\\nÚltimo threshold utilizado: {best_threshold_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARACIÓN: MODELO ORIGINAL (UNIGRAMAS) vs MODELO MEJORADO (UNIGRAMAS + BI-GRAMAS)\n",
      "================================================================================\n",
      "  Métrica  Modelo Original (Unigramas)  Modelo Mejorado (Unigramas + Bi-gramas)  Mejora  % Cambio\n",
      " Accuracy                       0.9884                                   0.9894  0.0010    0.1000\n",
      "Precision                       0.9741                                   0.9664 -0.0078   -0.8000\n",
      "   Recall                       0.9262                                   0.9426  0.0164    1.7700\n",
      " F1 Score                       0.9496                                   0.9544  0.0048    0.5000\n",
      "\n",
      "================================================================================\n",
      "Análisis:\n",
      "  - Tamaño del vocabulario: 6324 → 7659 (+1335 características)\n",
      "  - Threshold original: 0.7 → Threshold bi-gramas: 0.7\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "## Comparison: Unigrams vs Unigrams + Bigrams\n",
    "\n",
    "print(\"COMPARACIÓN: MODELO ORIGINAL (UNIGRAMAS) vs MODELO MEJORADO (UNIGRAMAS + BI-GRAMAS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get metrics from both models\n",
    "original_metrics = metrics\n",
    "bigram_metrics = metrics_bigrams\n",
    "\n",
    "comparison_data = {\n",
    "    'Métrica': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Modelo Original (Unigramas)': [\n",
    "        original_metrics.loc['Accuracy', 'Metrics'],\n",
    "        original_metrics.loc['Precission', 'Metrics'],\n",
    "        original_metrics.loc['Recall', 'Metrics'],\n",
    "        original_metrics.loc['F1 Score', 'Metrics']\n",
    "    ],\n",
    "    'Modelo Mejorado (Unigramas + Bi-gramas)': [\n",
    "        bigram_metrics.loc['Accuracy', 'Metrics'],\n",
    "        bigram_metrics.loc['Precission', 'Metrics'],\n",
    "        bigram_metrics.loc['Recall', 'Metrics'],\n",
    "        bigram_metrics.loc['F1 Score', 'Metrics']\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Mejora'] = comparison_df['Modelo Mejorado (Unigramas + Bi-gramas)'] - comparison_df['Modelo Original (Unigramas)']\n",
    "comparison_df['% Cambio'] = (comparison_df['Mejora'] / comparison_df['Modelo Original (Unigramas)'] * 100).round(2)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Análisis:\")\n",
    "print(f\"  - Tamaño del vocabulario: {vocab_size} → {vocab_size_bigrams} (+{vocab_size_bigrams - vocab_size} características)\")\n",
    "print(f\"  - Threshold original: {best_threshold} → Threshold bi-gramas: {best_threshold_bigrams}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANÁLISIS DE BI-GRAMAS DESPUÉS DEL FILTRADO (min_freq = 3)\n",
      "======================================================================\n",
      "\n",
      "TOP 10 BI-GRAMAS MAS COMUNES EN SPAM (después del filtrado):\n",
      "----------------------------------------------------------------------\n",
      "  co_uk: 35 ocurrencias\n",
      "  pleas_call: 34 ocurrencias\n",
      "  contact_u: 24 ocurrencias\n",
      "  1_50: 22 ocurrencias\n",
      "  tri_contact: 20 ocurrencias\n",
      "  po_box: 19 ocurrencias\n",
      "  custom_servic: 16 ocurrencias\n",
      "  await_collect: 16 ocurrencias\n",
      "  prize_guarante: 16 ocurrencias\n",
      "  guarante_call: 16 ocurrencias\n",
      "\n",
      "\n",
      "TOP 10 BI-GRAMAS MAS COMUNES EN NO-SPAM (después del filtrado):\n",
      "----------------------------------------------------------------------\n",
      "  lt_gt: 184 ocurrencias\n",
      "  gon_na: 41 ocurrencias\n",
      "  take_care: 30 ocurrencias\n",
      "  r_u: 29 ocurrencias\n",
      "  let_know: 28 ocurrencias\n",
      "  wan_na: 24 ocurrencias\n",
      "  wan_2: 24 ocurrencias\n",
      "  u_r: 24 ocurrencias\n",
      "  good_morn: 23 ocurrencias\n",
      "  k_k: 22 ocurrencias\n",
      "\n",
      "\n",
      "Bi-gramas con MAYOR PROBABILIDAD EN SPAM (filtrados):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Bi-gramas con MAYOR PROBABILIDAD EN NO-SPAM (filtrados):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "IMPACTO DEL FILTRADO\n",
      "======================================================================\n",
      "Bi-gramas totales en SPAM (antes): 7965\n",
      "Bi-gramas en SPAM después del filtrado: 713\n",
      "Reducción: 7252 (91.0%)\n",
      "\n",
      "Bi-gramas totales en NO-SPAM (antes): 25975\n",
      "Bi-gramas en NO-SPAM después del filtrado: 755\n",
      "Reducción: 25220 (97.1%)\n"
     ]
    }
   ],
   "source": [
    "## Bi-gram Analysis: Most Important Features (with frequency filtering)\n",
    "\n",
    "# Find the most common bi-grams in spam and non-spam emails (only those that passed the filter)\n",
    "spam_bigrams_filtered = [word for email in spam_emails_bigrams['email'] for word in email \n",
    "                         if '_' in word and word in filtered_bigrams_set]\n",
    "non_spam_bigrams_filtered = [word for email in non_spam_emails_bigrams['email'] for word in email \n",
    "                             if '_' in word and word in filtered_bigrams_set]\n",
    "\n",
    "# Count frequencies\n",
    "from collections import Counter\n",
    "\n",
    "spam_bigram_freq_filtered = Counter(spam_bigrams_filtered)\n",
    "non_spam_bigram_freq_filtered = Counter(non_spam_bigrams_filtered)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISIS DE BI-GRAMAS DESPUÉS DEL FILTRADO (min_freq = 3)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"TOP 10 BI-GRAMAS MAS COMUNES EN SPAM (después del filtrado):\")\n",
    "print(\"-\" * 70)\n",
    "for bigram, count in spam_bigram_freq_filtered.most_common(10):\n",
    "    print(f\"  {bigram}: {count} ocurrencias\")\n",
    "\n",
    "print(\"\\n\\nTOP 10 BI-GRAMAS MAS COMUNES EN NO-SPAM (después del filtrado):\")\n",
    "print(\"-\" * 70)\n",
    "for bigram, count in non_spam_bigram_freq_filtered.most_common(10):\n",
    "    print(f\"  {bigram}: {count} ocurrencias\")\n",
    "\n",
    "print(\"\\n\\nBi-gramas con MAYOR PROBABILIDAD EN SPAM (filtrados):\")\n",
    "print(\"-\" * 70)\n",
    "spam_bigram_probs_filtered = {word: prob for word, prob in probability_spam_words_bigrams.items() \n",
    "                               if '_' in word and word in filtered_bigrams_set}\n",
    "top_spam_bigrams_filtered = sorted(spam_bigram_probs_filtered.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for bigram, prob in top_spam_bigrams_filtered:\n",
    "    print(f\"  {bigram}: P(bi-grama|spam) = {prob:.6f}\")\n",
    "\n",
    "print(\"\\n\\nBi-gramas con MAYOR PROBABILIDAD EN NO-SPAM (filtrados):\")\n",
    "print(\"-\" * 70)\n",
    "non_spam_bigram_probs_filtered = {word: prob for word, prob in probability_non_spam_words_bigrams.items() \n",
    "                                   if '_' in word and word in filtered_bigrams_set}\n",
    "top_non_spam_bigrams_filtered = sorted(non_spam_bigram_probs_filtered.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for bigram, prob in top_non_spam_bigrams_filtered:\n",
    "    print(f\"  {bigram}: P(bi-grama|no-spam) = {prob:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPACTO DEL FILTRADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Bi-gramas totales en SPAM (antes): {len(spam_bow_bigrams)}\")\n",
    "print(f\"Bi-gramas en SPAM después del filtrado: {len(spam_bigram_freq_filtered)}\")\n",
    "print(f\"Reducción: {len(spam_bow_bigrams) - len(spam_bigram_freq_filtered)} ({100*(len(spam_bow_bigrams) - len(spam_bigram_freq_filtered))/len(spam_bow_bigrams):.1f}%)\")\n",
    "print()\n",
    "print(f\"Bi-gramas totales en NO-SPAM (antes): {len(non_spam_bow_bigrams)}\")\n",
    "print(f\"Bi-gramas en NO-SPAM después del filtrado: {len(non_spam_bigram_freq_filtered)}\")\n",
    "print(f\"Reducción: {len(non_spam_bow_bigrams) - len(non_spam_bigram_freq_filtered)} ({100*(len(non_spam_bow_bigrams) - len(non_spam_bigram_freq_filtered))/len(non_spam_bow_bigrams):.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
