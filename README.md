# MA2014 - MÃ©todos de Razonamiento e Incertidumbre

![Python](https://img.shields.io/badge/Python-3.x-blue)
![Machine Learning](https://img.shields.io/badge/Machine%20Learning-Probabilistic%20Models-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ“– DescripciÃ³n

Este repositorio contiene el material del curso **MA2014 - MÃ©todos de Razonamiento e Incertidumbre**, enfocado en modelos probabilÃ­sticos y tÃ©cnicas de machine learning para el razonamiento bajo incertidumbre. El curso cubre desde clasificadores bÃ¡sicos hasta modelos secuenciales y sistemas de toma de decisiones.

---

## ğŸ“ Estructura del Repositorio

```
MA2014/
â”œâ”€â”€ MÃ³dulo I/
â”‚   â”œâ”€â”€ Naive Bayes Classifier.ipynb
â”‚   â””â”€â”€ spam.csv
â”œâ”€â”€ MÃ³dulo II/
â”‚   â”œâ”€â”€ Bayesian Networks.ipynb
â”‚   â”œâ”€â”€ Solution Template.ipynb
â”‚   â””â”€â”€ diabetes-dataset.csv
â”œâ”€â”€ MÃ³dulo III/
â”‚   â”œâ”€â”€ Hidden Markov Models.ipynb
â”‚   â””â”€â”€ Solution Template.ipynb
â”œâ”€â”€ MÃ³dulo IV/
â”‚   â”œâ”€â”€ Solution Template.ipynb
â”‚   â””â”€â”€ diabetes-dataset.csv
â””â”€â”€ README.md
```

---

## ğŸ“š Contenido por MÃ³dulo

### ğŸ”¹ MÃ³dulo I: Clasificador Naive Bayes

**Tema:** ImplementaciÃ³n y EvaluaciÃ³n de un Clasificador Naive Bayes para la DetecciÃ³n AutomÃ¡tica de Correos ElectrÃ³nicos SPAM

#### Conceptos Cubiertos:
- **Teorema de Bayes** - Fundamento matemÃ¡tico del clasificador
- **Probabilidad Condicional** - $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
- **Supuesto de Independencia Naive** - Independencia condicional entre caracterÃ­sticas
- **Procesamiento de Lenguaje Natural (NLP)**:
  - TokenizaciÃ³n
  - EliminaciÃ³n de stopwords
  - Stemming y LemmatizaciÃ³n
- **Modelo Bag of Words** - RepresentaciÃ³n de documentos por frecuencia de palabras
- **N-gramas (Bi-gramas)** - Captura de secuencias de palabras

#### MÃ©tricas de EvaluaciÃ³n:
| MÃ©trica | FÃ³rmula |
|---------|---------|
| **Accuracy** | $\frac{TP + TN}{TP + FP + TN + FN}$ |
| **Precision** | $\frac{TP}{TP + FP}$ |
| **Recall** | $\frac{TP}{TP + FN}$ |
| **F1 Score** | $2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$ |

#### Dataset:
- **UCI ML SMS Spam Collection** (~5,500 mensajes)
- Variables: Texto del email, Etiqueta (spam/ham)

#### LibrerÃ­as Utilizadas:
```python
pandas, numpy, nltk, re
```

---

### ğŸ”¹ MÃ³dulo II: Redes Bayesianas

**Tema:** ImplementaciÃ³n de Redes Bayesianas para modelar dependencias probabilÃ­sticas

#### Conceptos Cubiertos:
- **Redes Bayesianas (Bayesian Belief Networks)** - Grafos dirigidos acÃ­clicos (DAGs)
- **Tablas de Probabilidad Condicional (CPT)** - $P(X|Pa(X))$
- **Inferencia ProbabilÃ­stica** - ActualizaciÃ³n de creencias con evidencia
- **Problema de Monty Hall** - Ejemplo clÃ¡sico de inferencia bayesiana

#### Estructura de una Red Bayesiana:
```
Nodos â†’ Variables aleatorias
Aristas â†’ Dependencias probabilÃ­sticas
CPTs â†’ Probabilidades condicionales
```

#### Proyecto: DiagnÃ³stico de Diabetes
- **Objetivo:** Predecir probabilidad de diabetes dado factores de salud
- **Variables:** Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, Age, Outcome
- **DiscretizaciÃ³n:** Por cuartiles (Q1, Q2, Q3, Q4) â†’ valores {0, 1, 2, 3}

#### LibrerÃ­as Utilizadas:
```python
pybbn, networkx, matplotlib, pandas
```

---

### ğŸ”¹ MÃ³dulo III: Modelos Ocultos de Markov (HMM)

**Tema:** Hidden Markov Models para inferencia en secuencias temporales

#### Conceptos Cubiertos:
- **Cadenas de Markov** - Propiedad de Markov (memoryless)
- **Modelos Ocultos de Markov** - Estados ocultos + observaciones
- **Matriz de TransiciÃ³n** - $T_{ij} = P(X_k = s_j | X_{k-1} = s_i)$
- **Matrices de ObservaciÃ³n** - $O_{j,ii} = P(E_k = e_j | X_k = s_i)$

#### Algoritmos de Inferencia:

| Algoritmo | Objetivo | FÃ³rmula |
|-----------|----------|---------|
| **Filtering** | $P(X_k \| E_{1:k})$ | $f_{1:k+1} = \alpha \cdot O[k+1] \cdot T^T \cdot f_{0:k}$ |
| **Prediction** | $P(X_{k+j} \| E_{1:k})$ | $(T^T)^j \cdot filtering(E)$ |
| **Smoothing** | $P(X_j \| E_{1:k})$ para $j < k$ | $forward \cdot backward$ |
| **Viterbi** | Secuencia mÃ¡s probable | $\max_{x_{1:k}} P(x_{1:k} \| E_{1:k})$ |

#### Ejemplo: Problema del Paraguas
- **Estados ocultos:** Rain, No Rain
- **Observaciones:** Umbrella, No Umbrella
- **Objetivo:** Inferir el clima dado las observaciones

#### Proyecto: LocalizaciÃ³n de Robot
- **Escenario:** Robot en una cuadrÃ­cula con obstÃ¡culos
- **Sensores:** 4 direcciones (N, E, W, S) con tasa de error $\epsilon$
- **Objetivo:** Inferir posiciÃ³n del robot mediante filtering/smoothing/Viterbi
- **VisualizaciÃ³n:** Heatmaps de probabilidad

#### LibrerÃ­as Utilizadas:
```python
numpy, numpy.linalg
```

---

### ğŸ”¹ MÃ³dulo IV: TeorÃ­a de la DecisiÃ³n y Diagramas de Influencia

**Tema:** ExtensiÃ³n de Redes Bayesianas con nodos de decisiÃ³n y utilidad

#### Conceptos Cubiertos:
- **Diagramas de Influencia** - Redes Bayesianas + decisiones + utilidades
- **FunciÃ³n de Utilidad** - $U(estado, acciÃ³n)$
- **Utilidad Esperada** - $EU(a) = \sum_s P(s|E) \cdot U(s, a)$
- **Principio de MÃ¡xima Utilidad Esperada (MEU)** - $a^* = \arg\max_a EU(a)$

#### Estructura del Diagrama de Influencia:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nodos de Azar (CÃ­rculos) â†’ Variables aleatorias     â”‚
â”‚  Nodos de DecisiÃ³n (RectÃ¡ngulos) â†’ Acciones          â”‚
â”‚  Nodos de Utilidad (Diamantes) â†’ Valores de utilidad â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Proyecto: RecomendaciÃ³n de Dieta
- **DecisiÃ³n:** Elegir alimento segÃºn Ã­ndice glucÃ©mico (GI)
  - Low GI (< 55)
  - Medium GI (55-69)
  - High GI (â‰¥ 70)
- **Estados:** DiabÃ©tico (1) / No DiabÃ©tico (0)

**Tabla de Utilidad:**

| Estado \ AcciÃ³n | Low GI | Medium GI | High GI |
|-----------------|--------|-----------|---------|
| No DiabÃ©tico (0) | 60 | 100 | 80 |
| DiabÃ©tico (1) | 100 | 50 | 0 |

**Resultados Esperados:**
- No diabÃ©tico (P=1.0) â†’ **Medium GI** (U=100)
- DiabÃ©tico (P=1.0) â†’ **Low GI** (U=100)
- Incierto (P=0.5) â†’ **Low GI** (EU=80)

#### LibrerÃ­as Utilizadas:
```python
pybbn, networkx, matplotlib, pandas, numpy
```

---

## ğŸ› ï¸ InstalaciÃ³n

### Requisitos Previos
```bash
pip install pandas numpy matplotlib networkx nltk pybbn
```

### Descargar recursos de NLTK
```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')
```

---

## ğŸ“Š Datasets Utilizados

| Dataset | MÃ³dulo | DescripciÃ³n | Fuente |
|---------|--------|-------------|--------|
| `spam.csv` | I | SMS Spam Collection (~5,500 mensajes) | [UCI ML Repository](https://www.kaggle.com/uciml/sms-spam-collection-dataset) |
| `diabetes-dataset.csv` | II, IV | Variables de salud para predicciÃ³n de diabetes | [Kaggle](https://www.kaggle.com/vikasukani/diabetes-data-set) |

---

## ğŸ§® FÃ³rmulas MatemÃ¡ticas Clave

### Teorema de Bayes
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

### Clasificador Naive Bayes
$$P(spam|w_1, w_2, ..., w_n) = \frac{\prod_{i=1}^{n} P(w_i|spam) \cdot P(spam)}{P(w_1, w_2, ..., w_n)}$$

### Suavizado de Laplace
$$P(w|clase) = \frac{count(w) + \alpha}{total\_words + \alpha \cdot vocab\_size}$$

### Filtering (HMM)
$$f_{1:k+1} = \alpha \cdot O[k+1] \cdot T^T \cdot f_{0:k}$$

### Smoothing (HMM)
$$P(X_j|E_{1:k}) \propto f_{1:j} \cdot b_{j+1:k}$$

### Utilidad Esperada
$$EU(a) = \sum_{s \in S} P(s|E) \cdot U(s, a)$$

---

## ğŸ“ˆ Flujo de Trabajo TÃ­pico

```mermaid
graph LR
    A[Datos] --> B[Preprocesamiento]
    B --> C[Modelado]
    C --> D[Entrenamiento]
    D --> E[Inferencia]
    E --> F[EvaluaciÃ³n]
    F --> G[DecisiÃ³n]
```

---

## ğŸ“ Referencias

- **Naive Bayes:** [NLTK Documentation](https://www.nltk.org/)
- **Redes Bayesianas:** [PyBBN Documentation](https://py-bbn.readthedocs.io/)
- **HMM:** Russell, S. & Norvig, P. - *Artificial Intelligence: A Modern Approach*
- **TeorÃ­a de la DecisiÃ³n:** Mohammadi et al. (2015) - *Using Bayesian Network for the Prediction and Diagnosis of Diabetes*

---

## ğŸ‘¤ Autor

**Ricardo B.**  
Instituto TecnolÃ³gico y de Estudios Superiores de Monterrey  
Ciencia de Datos

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.
